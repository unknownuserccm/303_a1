{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Classification with VGG16 - Google Colab\n",
    "\n",
    "This notebook trains a VGG16 model on fashion images.\n",
    "\n",
    "**Dataset Structure Required:**\n",
    "```\n",
    "ict303_a1/\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "    ‚îî‚îÄ‚îÄ data/\n",
    "        ‚îú‚îÄ‚îÄ train/\n",
    "        ‚îú‚îÄ‚îÄ valid/\n",
    "        ‚îî‚îÄ‚îÄ test/unknown/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "project_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
    "os.chdir(project_dir)\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(\"\\nFiles in directory:\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print('\\n‚úÖ GPU is available!')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  GPU not available. Go to Runtime ‚Üí Change runtime type ‚Üí GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Python Files are Uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all required Python files exist\n",
    "required_files = [\n",
    "    'dataset.py',\n",
    "    'trainer.py',\n",
    "    'vgg16_pretrained.py',\n",
    "    'vgg16_scratch.py',\n",
    "    'mlp_model.py',\n",
    "    'utils.py'\n",
    "]\n",
    "\n",
    "print(\"Checking for required files...\\n\")\n",
    "all_found = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file} found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} NOT FOUND - Please upload this file!\")\n",
    "        all_found = False\n",
    "\n",
    "if all_found:\n",
    "    print(\"\\n‚úÖ All required files found!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some files are missing. Please upload them to the same folder as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Dataset Path and Verify Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THIS PATH to where your ict303_a1 folder is located\n",
    "# Common options:\n",
    "# Option 1: In Colab Notebooks folder\n",
    "root_dir = \"/content/drive/MyDrive/Colab Notebooks/ICT303 - Fashion Classification\"\n",
    "\n",
    "# Option 2: At root of Drive  \n",
    "# root_dir = \"/content/drive/MyDrive/ICT303 - Fashion Classification\"\n",
    "\n",
    "# Option 3: In a specific folder\n",
    "# root_dir = \"/content/drive/MyDrive/YourFolder/ICT303 - Fashion Classification\"\n",
    "\n",
    "print(f\"Dataset root directory: {root_dir}\")\n",
    "print(f\"Path exists: {os.path.exists(root_dir)}\\n\")\n",
    "\n",
    "if os.path.exists(root_dir):\n",
    "    # Check structure\n",
    "    data_data_path = os.path.join(root_dir, 'data', 'data')\n",
    "    \n",
    "    if os.path.exists(data_data_path):\n",
    "        print(f\"‚úÖ Found data/data/ structure\")\n",
    "        \n",
    "        # Check for train, valid, test\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            split_path = os.path.join(data_data_path, split)\n",
    "            if os.path.exists(split_path):\n",
    "                print(f\"  ‚úÖ {split}/ found\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {split}/ NOT FOUND\")\n",
    "    else:\n",
    "        print(f\"‚ùå data/data/ structure not found\")\n",
    "        print(f\"\\nContents of {root_dir}:\")\n",
    "        !ls -la \"$root_dir\"\n",
    "else:\n",
    "    print(\"‚ùå Root directory not found!\")\n",
    "    print(\"\\nSearching for ict303_a1 or ICT303 folders...\")\n",
    "    !find /content/drive/MyDrive -name \"*303*\" -type d 2>/dev/null | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataset import get_dataloaders\n",
    "from trainer import Trainer\n",
    "from vgg16_pretrained import create_vgg16_pretrained\n",
    "from vgg16_scratch import VGG16Scratch\n",
    "from mlp_model import create_mlp_model\n",
    "import utils\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batch_size = 32\n",
    "img_size = 224\n",
    "\n",
    "train_loader, val_loader, test_loader, dataset_info = get_dataloaders(\n",
    "    root_dir=root_dir,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    num_workers=2  # Good for Colab\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Number of classes: {dataset_info['num_classes']}\")\n",
    "print(f\"Classes: {dataset_info['classes']}\")\n",
    "print(f\"Training samples: {dataset_info['train_size']}\")\n",
    "print(f\"Validation samples: {dataset_info['val_size']}\")\n",
    "print(f\"Test samples: {dataset_info['test_size']}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify data loading\n",
    "if dataset_info['train_size'] > 0:\n",
    "    images, labels = next(iter(train_loader))\n",
    "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Sample batch shape: {images.shape}\")\n",
    "    print(f\"Sample labels: {labels[:5]}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No training data found! Please check your root_dir path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model\n",
    "\n",
    "### Choose ONE of the following options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: VGG16 Pretrained (RECOMMENDED - Best Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VGG16 Pretrained model\n",
    "model = create_vgg16_pretrained(\n",
    "    model_type='standard',  # 'standard', 'small_classifier', or 'global_pool'\n",
    "    num_classes=8,\n",
    "    freeze_features=True,  # Freeze conv layers, train only classifier\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: VGG16 Pretrained\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: VGG16 from Scratch (Slower, needs more epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use VGG16 from scratch\n",
    "# model = VGG16Scratch(\n",
    "#     num_classes=8,\n",
    "#     dropout_rate=0.5,\n",
    "#     use_batch_norm=False\n",
    "# )\n",
    "# model = model.to(device)\n",
    "# print(f\"Model: VGG16 from Scratch\")\n",
    "# print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: MLP Model (Fastest, lower performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use MLP\n",
    "# model = create_mlp_model(\n",
    "#     model_size='medium',\n",
    "#     input_size=224*224*3,\n",
    "#     num_classes=8,\n",
    "#     dropout_rate=0.5\n",
    "# )\n",
    "# model = model.to(device)\n",
    "# print(f\"Model: MLP Medium\")\n",
    "# print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (use lower learning rate for pretrained models)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler,\n",
    "    log_dir='./runs/vgg16_pretrained',\n",
    "    class_names=dataset_info['classes']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "os.makedirs('./checkpoints/vgg16_pretrained', exist_ok=True)\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(\n",
    "    num_epochs=25,  # Adjust as needed\n",
    "    save_dir='./checkpoints/vgg16_pretrained',\n",
    "    early_stopping_patience=10  # Stop if no improvement for 10 epochs\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Best validation accuracy: {trainer.best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Evaluating on validation set...\")\n",
    "\n",
    "# Load best model\n",
    "trainer.load_checkpoint(trainer.best_model_path)\n",
    "\n",
    "# Evaluate\n",
    "metrics = trainer.evaluate(val_loader, return_predictions=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {metrics['map']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "# Plot training curves\n",
    "trainer.plot_training_curves(save_path='./results/training_curves.png')\n",
    "print(\"‚úÖ Training curves saved\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "trainer.plot_confusion_matrix(\n",
    "    metrics['confusion_matrix'],\n",
    "    save_path='./results/confusion_matrix.png'\n",
    ")\n",
    "print(\"‚úÖ Confusion matrix saved\")\n",
    "\n",
    "# Display images\n",
    "from IPython.display import Image, display\n",
    "print(\"\\nTraining Curves:\")\n",
    "display(Image('./results/training_curves.png'))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "display(Image('./results/confusion_matrix.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_predictions\n",
    "\n",
    "visualize_predictions(\n",
    "    model=model,\n",
    "    data_loader=val_loader,\n",
    "    device=device,\n",
    "    class_names=dataset_info['classes'],\n",
    "    num_images=16,\n",
    "    save_path='./results/predictions.png'\n",
    ")\n",
    "\n",
    "print(\"\\nPrediction Samples:\")\n",
    "display(Image('./results/predictions.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_per_class_accuracy, plot_per_class_accuracy\n",
    "\n",
    "per_class_acc = compute_per_class_accuracy(\n",
    "    metrics['predictions'],\n",
    "    metrics['labels'],\n",
    "    dataset_info['classes']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*70)\n",
    "for class_name, acc in per_class_acc.items():\n",
    "    print(f\"{class_name:15s}: {acc:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot\n",
    "plot_per_class_accuracy(per_class_acc, save_path='./results/per_class_accuracy.png')\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "display(Image('./results/per_class_accuracy.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_classification_report\n",
    "\n",
    "print_classification_report(\n",
    "    metrics['predictions'],\n",
    "    metrics['labels'],\n",
    "    dataset_info['classes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÆ Making predictions on test set...\")\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Convert to class names\n",
    "predicted_classes = [dataset_info['classes'][pred] for pred in test_predictions]\n",
    "\n",
    "print(f\"‚úÖ Made predictions for {len(test_predictions)} test images\")\n",
    "\n",
    "# Save predictions\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'image_id': range(len(test_predictions)),\n",
    "    'predicted_class': predicted_classes,\n",
    "    'predicted_index': test_predictions\n",
    "})\n",
    "\n",
    "results_df.to_csv('./results/test_predictions.csv', index=False)\n",
    "print(\"‚úÖ Test predictions saved to: ./results/test_predictions.csv\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model.state_dict(), './final_model.pth')\n",
    "print(\"‚úÖ Final model saved to: ./final_model.pth\")\n",
    "\n",
    "print(\"\\nüéâ ALL DONE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"Results saved in './results/' folder:\")\n",
    "print(\"  - training_curves.png\")\n",
    "print(\"  - confusion_matrix.png\")\n",
    "print(\"  - predictions.png\")\n",
    "print(\"  - per_class_accuracy.png\")\n",
    "print(\"  - test_predictions.csv\")\n",
    "print(\"\\nBest model saved in './checkpoints/vgg16_pretrained/best_model.pth'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all results for easy download\n",
    "!zip -r results.zip results/ checkpoints/ *.pth\n",
    "\n",
    "print(\"‚úÖ Results zipped!\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Click on the folder icon on the left\")\n",
    "print(\"2. Right-click on 'results.zip'\")\n",
    "print(\"3. Select 'Download'\")\n",
    "\n",
    "# Or use Colab's download function\n",
    "from google.colab import files\n",
    "# Uncomment to download directly:\n",
    "# files.download('results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. View TensorBoard Logs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
